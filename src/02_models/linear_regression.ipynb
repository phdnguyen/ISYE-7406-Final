{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "634da99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/23 03:25:19 WARN Utils: Your hostname, Phongs-MacBook-Pro-23.local, resolves to a loopback address: 127.0.0.1; using 10.0.0.46 instead (on interface en0)\n",
      "25/11/23 03:25:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/23 03:25:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/23 03:25:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/23 03:25:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Set up Spark Session\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Seattle911\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e1ade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 848,167 rows with 22 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. Data Import\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"../../\").resolve()\n",
    "\n",
    "data_path = PROJECT_ROOT / \"data\" / \"processed\" / \"calldata_20251019_processed.parquet\"\n",
    "\n",
    "df = spark.read.parquet(str(data_path))\n",
    "print(f\"Loaded {df.count():,} rows with {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23600db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. Data Filtering and Feature Setup\n",
    "# ============================================================\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "label_col = \"First SPD Call Sign Response Time (s)\"\n",
    "\n",
    "\n",
    "raw_feats = [\n",
    "    'Priority',\n",
    "    'Dispatch Neighborhood',\n",
    "    'Dispatch Sector',\n",
    "    'Count Of Officers',\n",
    "    'is_rush_hour',\n",
    "    'is_nighttime',\n",
    "    'priority_x_officers',\n",
    "    'priority_x_hour',\n",
    "    'event_ts',\n",
    "    'year',\n",
    "    'month',\n",
    "    'day',\n",
    "    'day_of_week',\n",
    "    'hour',\n",
    "    'TEMP',\n",
    "    'PRCP',\n",
    "    'COCO',\n",
    "    'weather_severity',\n",
    "    'is_raining',\n",
    "    'is_freezing'\n",
    "]\n",
    "\n",
    "num_feats = [\n",
    "    'Priority',\n",
    "    'Count Of Officers',\n",
    "    'is_rush_hour',\n",
    "    'is_nighttime',\n",
    "    'priority_x_officers',\n",
    "    'priority_x_hour',\n",
    "    'year',\n",
    "    'month',\n",
    "    'day',\n",
    "    'day_of_week',\n",
    "    'hour',\n",
    "    'TEMP',\n",
    "    'PRCP',\n",
    "    'weather_severity',\n",
    "    'is_raining',\n",
    "    'is_freezing'\n",
    "]\n",
    "\n",
    "cat_feats = [\"Dispatch Neighborhood\", \"Dispatch Sector\"]\n",
    "\n",
    "def to_d(c):\n",
    "    return F.coalesce(F.col(c).cast(\"double\"), F.lit(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bd76dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                        (0 + 11) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 678,581, Test size: 169,586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Pipeline: Index â†’ OneHot â†’ Assemble\n",
    "# ============================================================\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in cat_feats\n",
    "]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCols=[f\"{c}_idx\"], outputCols=[f\"{c}_vec\"])\n",
    "    for c in cat_feats\n",
    "]\n",
    "assembled_feats = [f\"{c}_vec\" for c in cat_feats] + num_feats\n",
    "assembler = VectorAssembler(inputCols=assembled_feats, outputCol=\"features\")\n",
    "\n",
    "# Label column\n",
    "df_labeled = df.withColumn(\"label\", F.col(label_col)).dropna(subset=[\"label\"])\n",
    "\n",
    "# Split train/test (CV runs only on train)\n",
    "train, test = df_labeled.randomSplit([0.8, 0.2], seed=42)\n",
    "train = train.cache()\n",
    "print(f\"Train size: {train.count():,}, Test size: {test.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f98153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. Linear Regression Baseline Model\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=50,\n",
    "    regParam=0.0,\n",
    "    elasticNetParam=0.0,\n",
    "    standardization=True\n",
    ")\n",
    "\n",
    "lr_pipeline = Pipeline(stages=indexers + encoders + [assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dffbdd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 03:25:29 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/23 03:25:29 WARN Instrumentation: [a6b152bc] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/11/23 03:25:30 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/23 03:25:30 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "25/11/23 03:25:30 WARN Instrumentation: [a6b152bc] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. Fit LR Model\n",
    "# ============================================================\n",
    "\n",
    "lr_model = lr_pipeline.fit(train)\n",
    "lr_pred = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ce44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Linear Regression Results:\n",
      "MAE:   1,003.98\n",
      "RMSE:  1,375.95\n",
      "MSE:   1,893,248.21\n",
      "RÂ²:    0.1460\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. Evaluate LR Model\n",
    "# ============================================================\n",
    "\n",
    "lr_mae = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\"\n",
    ").evaluate(lr_pred)\n",
    "\n",
    "lr_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    ").evaluate(lr_pred)\n",
    "\n",
    "lr_mse = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\"\n",
    ").evaluate(lr_pred)\n",
    "\n",
    "lr_r2 = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\"\n",
    ").evaluate(lr_pred)\n",
    "\n",
    "print(\"\\n Linear Regression Results:\")\n",
    "print(f\"MAE:   {lr_mae:,.2f}\")\n",
    "print(f\"RMSE:  {lr_rmse:,.2f}\")\n",
    "print(f\"MSE:   {lr_mse:,.2f}\")\n",
    "print(f\"RÂ²:    {lr_r2:,.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6238d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intercept: -98899.0930877213\n",
      "Number of coefficients: 94\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. Inspect LR Stage\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "lr_stage = next(\n",
    "    s for s in lr_model.stages if isinstance(s, LinearRegressionModel)\n",
    ")\n",
    "\n",
    "print(\"\\nIntercept:\", lr_stage.intercept)\n",
    "print(\"Number of coefficients:\", len(lr_stage.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a841af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seattledata1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
